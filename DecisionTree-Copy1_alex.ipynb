{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d8342ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a50ff0",
   "metadata": {},
   "source": [
    "# LOAD THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44917e",
   "metadata": {},
   "source": [
    "- last column of df , wich is the  the variable of interest for classification will be renamed as \"label\";\n",
    "\n",
    "- remove the missing values in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bdafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53183af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b90904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_colunas_irrelevantes(df, limite_percentual, palavras_chave_id=['id']):\n",
    "    df_limpo = df.copy()\n",
    "    colunas_a_remover = []\n",
    "\n",
    "    for coluna in df.columns:\n",
    "        # Cálculo da proporção de valores únicos\n",
    "        proporcao_valores_unicos = df[coluna].nunique() / df[coluna].count()\n",
    "    \n",
    "\n",
    "        # Verificação da proporção e do nome da coluna\n",
    "        if proporcao_valores_unicos > limite_percentual:\n",
    "            colunas_a_remover.append(coluna)\n",
    "            \n",
    "        \n",
    "        # Verificação se o nome da coluna contém alguma das palavras chave para ID\n",
    "        coluna_lower = coluna.lower()\n",
    "        if any(re.fullmatch(rf'\\b{palavra}\\b', coluna_lower) for palavra in palavras_chave_id):\n",
    "            if coluna not in colunas_a_remover:\n",
    "                colunas_a_remover.append(coluna)\n",
    "                \n",
    "\n",
    "    # Remove as colunas identificadas\n",
    "    df_limpo.drop(colunas_a_remover, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    return df_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39522b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acd9e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_data_matrix(df):\n",
    "    return df.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92889d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c083adb",
   "metadata": {},
   "source": [
    "# TRAIN AND TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c42c7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size):\n",
    "    # If test_size is a float, convert it to an integer representing the number of test samples\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = int(round(test_size * len(df)))\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    random.shuffle(df)\n",
    "    \n",
    "    # Calculate the split index using the integer test_size\n",
    "    split_index = len(df) - test_size\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    train_data = df[:split_index]\n",
    "    test_data = df[split_index:]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c629975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83ee1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data_matrix, k):\n",
    "    # Shuffle data_matrix in place\n",
    "    random.shuffle(data_matrix)\n",
    "    \n",
    "    fold_size = len(data_matrix) // k\n",
    "    folds = []\n",
    "    \n",
    "    # Create the k folds\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        if i == k - 1:  # Ensure the last fold includes the remainder\n",
    "            end = len(data_matrix)\n",
    "        else:\n",
    "            end = start + fold_size\n",
    "\n",
    "        # Select the test set as the current fold\n",
    "        test_set = data_matrix[start:end]\n",
    "        \n",
    "        # Create training set as all the data except the current test set\n",
    "        train_set = data_matrix[:start] + data_matrix[end:]\n",
    "        \n",
    "        folds.append((train_set, test_set))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d71680ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncv_folds = k_fold_cross_validation(data_matrix, k=5)\\n\\nfor idx, (train, test) in enumerate(cv_folds):\\n    print(f\"Fold {idx+1}\")\\n    print(\"Train Set:\")\\n    print(train)\\n    print(\"Test Set:\")\\n    print(test)\\n    print(\"-\" * 40)\\n    \\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cv_folds = k_fold_cross_validation(data_matrix, k=5)\n",
    "\n",
    "for idx, (train, test) in enumerate(cv_folds):\n",
    "    print(f\"Fold {idx+1}\")\n",
    "    print(\"Train Set:\")\n",
    "    print(train)\n",
    "    print(\"Test Set:\")\n",
    "    print(test)\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ed33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df33fb",
   "metadata": {},
   "source": [
    "# CLASS NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "036f4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attribute = None, label = None, is_leaf = False, count = {}):\n",
    "        self.attribute = attribute\n",
    "        self.label = label\n",
    "        self.is_leaf = is_leaf\n",
    "        self.count = {}\n",
    "        self.children = {}\n",
    "\n",
    "    def recursive(self, node, space):\n",
    "        result = \"\"\n",
    "        if node.is_leaf:\n",
    "                if isinstance(node.count, dict):\n",
    "                    count_str = \" \".join(f\"{count}\" for _, count in node.count.items())\n",
    "                else:\n",
    "                    count_str = str(node.count)\n",
    "                result += f\"{space}{node.label}({count_str})\\n\"\n",
    "        else:\n",
    "            sorted_children = sorted(node.children.items(), key = lambda x: x[0]) # ordenar os filhos por ordem crescente\n",
    "            for value, child in sorted_children: # percorrer os filhos\n",
    "                result += f\"{space}{node.attribute}:{value}\\n\"\n",
    "                result += self.recursive(child, space + \"  \")\n",
    "        return result\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.recursive(self, \" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf7685",
   "metadata": {},
   "source": [
    "# ENTROPY AND SPLITS CALCULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8180afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_classes(dataset):\n",
    "    class_counts = {}\n",
    "    for data in dataset:\n",
    "        label = data[-1]\n",
    "        if label in class_counts:\n",
    "            class_counts[label] += 1\n",
    "        else:\n",
    "            class_counts[label] = 1\n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e60377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_class(labels):\n",
    "    class_counts = {}\n",
    "    for label in labels:\n",
    "        if label not in class_counts:\n",
    "            class_counts[label] = 1\n",
    "        else:\n",
    "            class_counts[label] += 1\n",
    "    return max(class_counts,key = class_counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "394360e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data_label):\n",
    "    count_classes = {}\n",
    "    total = len(data_label)\n",
    "    entropy = 0\n",
    "\n",
    "    for value in data_label:\n",
    "        if value not in count_classes:\n",
    "            count_classes[value] = 1\n",
    "        else:\n",
    "            count_classes[value] += 1\n",
    "    for value in count_classes.values():\n",
    "        p = value / total\n",
    "        entropy += -p * math.log2(p)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ca218c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_entropy(data_matrix, attributes):\n",
    "    entropy_attributes = {}\n",
    "    for _, val in attributes.items():\n",
    "        unique_attributes = list(set(row[val] for row in data_matrix))\n",
    "        entropy_attribute = 0\n",
    "        for value in unique_attributes:\n",
    "            subset = []\n",
    "            for row in data_matrix:\n",
    "                if row[val] == value:\n",
    "                    subset.append(row[-1])\n",
    "            if subset:  # Ensure subset is not empty before calculating its entropy\n",
    "                subset_entropy = entropy(subset)\n",
    "                subset_probability = len(subset) / len(data_matrix)\n",
    "                entropy_attribute += subset_entropy * subset_probability\n",
    "        entropy_attributes[val] = entropy_attribute\n",
    "    return entropy_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e5aea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(data_matrix,attributes, entropy_class):\n",
    "        info_gain = None\n",
    "        best_value = 0\n",
    "        value = 0\n",
    "        for key, val in attributes.items():\n",
    "            entropy_class = entropy([linha[-1] for linha in data_matrix])\n",
    "            entropy_attributes = attribute_entropy(data_matrix, attributes)\n",
    "            value = entropy_class - entropy_attributes[val] # calcular o ganho de informação\n",
    "            if value > best_value: # se o ganho de informação for maior que o anterior\n",
    "                best_value = value # atualizar o melhor ganho de informação\n",
    "                info_gain = key # atualizar o melhor atributo\n",
    "        return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4a4c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_info_gain(dataset, label_menor, label_maior):\n",
    "        parent_entropy = entropy(dataset)\n",
    "        probabily_menor = len(label_menor) / len(dataset)\n",
    "        probabily_maior = len(label_maior) / len(dataset)\n",
    "        entropy_menor = entropy(label_menor)\n",
    "        entropy_maior = entropy(label_maior)\n",
    "        value_info_gain = parent_entropy - (probabily_menor * entropy_menor) - (probabily_maior * entropy_maior)\n",
    "        return value_info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bad4b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_split(data_matrix, atributes, best_attribute):\n",
    "    unique_values = list(set([linha[attributes[best_attribute]] for linha in data_matrix]))\n",
    "    best_split = ''\n",
    "    best_info_gain = -99999\n",
    "    \n",
    "    if len(unique_values) == 1:\n",
    "        return unique_values[0]\n",
    "    \n",
    "    for value in unique_values:\n",
    "        \n",
    "        is_bigger_subset = []\n",
    "        is_lower_subset = []\n",
    "        \n",
    "        for linha in data_matrix:\n",
    "            if float(linha[attributes[best_attribute]]) <= float(value):\n",
    "                is_bigger_subset.append(linha)\n",
    "            if float(linha[attributes[best_attribute]]) > float(value):\n",
    "                is_lower_subset.append(linha)\n",
    "                    \n",
    "        labels_menor = [linha[-1] for linha in is_bigger_subset]\n",
    "        labels_maior = [linha[-1] for linha in is_lower_subset]\n",
    "        \n",
    "        info_gains  = split_info_gain([linha[-1] for linha in data_matrix], labels_menor, labels_maior)\n",
    "        if info_gains > best_info_gain:\n",
    "            best_info_gain = info_gains\n",
    "            best_split = value\n",
    "            \n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66dcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "706a4bf6",
   "metadata": {},
   "source": [
    "# TREE IMPLEMENTATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1848b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_data_type(values):\n",
    "    \"\"\" Determine if the attribute values are numeric or categorical. \"\"\"\n",
    "    try:\n",
    "        [float(value) for value in values]\n",
    "        return 'numeric'\n",
    "    except ValueError:\n",
    "        return 'categorical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f6561cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, data_matrix, attributes, label_class):\n",
    "        self.dataset = data_matrix\n",
    "        self.classe = label_class # nome da coluna da classe\n",
    "        self.entropy_class = entropy([linha[-1] for linha in data_matrix]) # entropia da classe\n",
    "        self.entropy_atrributes = {} # dicionario com a entropia de cada atributo\n",
    "        self.attribute_to_index = {attr: i for i, attr in enumerate(attributes)}\n",
    "\n",
    "        self.attributes =   {}\n",
    "        self.reverse_attributes = {} # exemplo : {1: 'sepallength', 2: 'sepalwidth', 3: 'petallength', 4: 'petalwidth', 5: 'class'}\n",
    "        for key, position in attributes.items():\n",
    "            if  position != len(attributes) -1:\n",
    "                self.reverse_attributes[position] = key\n",
    "                self.attributes[key] = position\n",
    "\n",
    "        self.attribute_entropy = attribute_entropy(data_matrix, attributes)\n",
    "        self.root = self.build_tree(data_matrix, self.attributes) # construir a arvore de decisão\n",
    "\n",
    "    def build_tree(self, data_matrix, attributes):\n",
    "        labels = [linha[-1] for linha in data_matrix]  # Classes of examples\n",
    "\n",
    "        if len(set(labels)) == 1:\n",
    "            return Node(attribute=None, is_leaf=True, label=labels[0], count=_count_classes(data_matrix))\n",
    "\n",
    "        if not attributes:\n",
    "            return Node(attribute=None, is_leaf=True, label=most_common_class(labels), count=len(labels))\n",
    "\n",
    "        best_attribute = information_gain(data_matrix, attributes, self.entropy_class)\n",
    "        root = Node(best_attribute)\n",
    "        val = attributes[best_attribute]\n",
    "        unique_values = list(set(linha[val] for linha in self.dataset))\n",
    "\n",
    "        data_type = determine_data_type(unique_values)\n",
    "\n",
    "        if data_type == 'numeric':\n",
    "            best_split = calculate_best_split(data_matrix, attributes, best_attribute)\n",
    "            subset_lower = [linha for linha in data_matrix if float(linha[val]) <= float(best_split)]\n",
    "            subset_higher = [linha for linha in data_matrix if float(linha[val]) > float(best_split)]\n",
    "            new_attributes = attributes.copy()\n",
    "            del new_attributes[best_attribute]\n",
    "            root.children['<= ' + str(best_split)] = self.build_tree(subset_lower, new_attributes)\n",
    "            root.children['> ' + str(best_split)] = self.build_tree(subset_higher, new_attributes)\n",
    "        else:\n",
    "            for value in unique_values:\n",
    "                subset = [linha for linha in data_matrix if linha[val] == value]\n",
    "                if subset:\n",
    "                    new_attributes = attributes.copy()\n",
    "                    del new_attributes[best_attribute]\n",
    "                    child_node = self.build_tree(subset, new_attributes)\n",
    "                    root.children[value] = child_node\n",
    "                else:\n",
    "                    root.children[value] = Node(attribute=None, is_leaf=True, label=most_common_class(labels), count=0)\n",
    "\n",
    "        return root\n",
    "    \n",
    "    def tranform(self, data):\n",
    "        if not isinstance(data[0], list):  # Check if data is not already a list of lists\n",
    "            data = [data]  # Make it a list of lists if it's a single row\n",
    "\n",
    "        results = []\n",
    "        for row in data:\n",
    "            result = self.tranform_tree(self.root, row)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "    def tranform_tree(self, tree, row):\n",
    "        if tree.is_leaf:\n",
    "            return tree.label\n",
    "\n",
    "        #print(\"TREE.ATRIBUTTE = \", tree.attribute)\n",
    "        #print(\"ROW = \" , row)\n",
    "        #print(\"TREE.CHILDREN = \", tree.children)\n",
    "        \n",
    "        attribute_index = self.attribute_to_index[tree.attribute]  # Convert attribute name to index\n",
    "        attribute_value = row[attribute_index]\n",
    "        \n",
    "    \n",
    "\n",
    "        try:\n",
    "            attribute_value = float(attribute_value)\n",
    "            #print(str(attribute_value) + \" \" + \"Success\")\n",
    "            split_key = list(tree.children.keys())[0]\n",
    "            split_operator, split_value = split_key.split(' ')\n",
    "            subtree = tree.children\n",
    "\n",
    "            if split_operator == '<=':\n",
    "                try:\n",
    "                    if float(attribute_value) <= float(split_value):\n",
    "                        subtree = subtree['<= ' + split_value]\n",
    "                    else:\n",
    "                        subtree = subtree['> ' + split_value]\n",
    "                except ValueError:\n",
    "                    return None  # Non-numeric value, skip the comparison\n",
    "            elif split_operator == '>':\n",
    "                try:\n",
    "                    if float(attribute_value) > float(split_value):\n",
    "                        subtree = subtree['> ' + split_value]\n",
    "                    else:\n",
    "                        subtree = subtree['<= ' + split_value]\n",
    "                except ValueError:\n",
    "                    return None  # Non-numeric value, skip the comparison\n",
    "\n",
    "            return self.tranform_tree(subtree, row)\n",
    "        except ValueError:\n",
    "            if attribute_value in tree.children:\n",
    "                child = tree.children[attribute_value]\n",
    "                return self.tranform_tree(child, row)\n",
    "            else:\n",
    "                # Handle missing attribute value or unseen attribute value\n",
    "                return None\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fa64f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a1 a2 a3 a4 a5 a6 b1 b2 b3 b4  ... f4 f5 f6 g1 g2 g3 g4 g5 g6  class\n",
      "0  b  b  b  b  b  b  b  b  b  b  ...  b  b  b  b  b  b  b  b  b    win\n",
      "1  b  b  b  b  b  b  b  b  b  b  ...  b  b  b  b  b  b  b  b  b    win\n",
      "2  b  b  b  b  b  b  o  b  b  b  ...  b  b  b  b  b  b  b  b  b    win\n",
      "3  b  b  b  b  b  b  b  b  b  b  ...  b  b  b  b  b  b  b  b  b    win\n",
      "4  o  b  b  b  b  b  b  b  b  b  ...  b  b  b  b  b  b  b  b  b    win\n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def df_to_data_matrix(df):\n",
    "    return df.values.tolist()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('connect4.csv')\n",
    "print(data.head())\n",
    "\n",
    "# Assuming the class label is in the last column and the ID column is absent in your CSV\n",
    "\n",
    "\n",
    "attributes = {col: idx for idx, col in enumerate(data.columns[:-1])}\n",
    "data_matrix = df_to_data_matrix(data)\n",
    "\n",
    "# Initialize and build the tree\n",
    "#tree = Tree(data_matrix, attributes, df.columns[-1])\n",
    "#print(tree)\n",
    "#print(data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de773c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "640735ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def k_fold_cross_validation(data_matrix, k, attributes):\n",
    "    random.shuffle(data_matrix)\n",
    "    fold_size = len(data_matrix) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = len(data_matrix) if i == k - 1 else start + fold_size\n",
    "        test_set = data_matrix[start:end]\n",
    "        train_set = data_matrix[:start] + data_matrix[end:]\n",
    "\n",
    "        # Train the tree using train_set\n",
    "        tree = Tree(train_set, attributes, df.columns[-1])\n",
    "    \n",
    "        # Test the tree using test_set\n",
    "        correct_predictions = 0\n",
    "       \n",
    "        for row in test_set:\n",
    "            \n",
    "            \n",
    "            \n",
    "            prediction = tree.tranform([row[:-1]])  # Exclude the label\n",
    "            if prediction[0] == row[-1]:  # Compare the predicted label with the actual label\n",
    "                correct_predictions += 1\n",
    "\n",
    "        accuracy = correct_predictions / len(test_set)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    average_accuracy = sum(accuracies) / k\n",
    "    return average_accuracy , accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23f26eb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m average_accuracy, accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mk_fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(average_accuracy)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracies)\n",
      "Cell \u001b[1;32mIn[56], line 15\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[1;34m(data_matrix, k, attributes)\u001b[0m\n\u001b[0;32m     12\u001b[0m train_set \u001b[38;5;241m=\u001b[39m data_matrix[:start] \u001b[38;5;241m+\u001b[39m data_matrix[end:]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the tree using train_set\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m tree \u001b[38;5;241m=\u001b[39m Tree(train_set, attributes, \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Test the tree using test_set\u001b[39;00m\n\u001b[0;32m     18\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "average_accuracy, accuracies = k_fold_cross_validation(data_matrix, 3 , attributes)\n",
    "print(average_accuracy)\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def leave_one_out_cross_validation(data_matrix, attributes):\n",
    "    random.shuffle(data_matrix)  # Embaralhando para randomizar a ordem dos dados\n",
    "    accuracies = []\n",
    "\n",
    "    # Loop sobre o dataset inteiro, deixando um de fora cada vez para teste\n",
    "    for i in range(len(data_matrix)):\n",
    "        test_set = [data_matrix[i]]  # O conjunto de teste é apenas um elemento\n",
    "        train_set = data_matrix[:i] + data_matrix[i+1:]  # Todos os outros dados formam o conjunto de treino\n",
    "\n",
    "        # Treinar a árvore usando o conjunto de treino\n",
    "        tree = Tree(train_set, attributes, df.columns[-1])  # Supondo que a classe Tree e df estão definidos corretamente\n",
    "        \n",
    "        # Testar a árvore usando o conjunto de teste\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for row in test_set:\n",
    "            print(\"row:\", row)\n",
    "            prediction = tree.tranform([row[:-1]])  # Exclui a label dos dados de entrada\n",
    "            print(\"prediction:\", prediction[0])\n",
    "            print(\"actual:\", row[-1])\n",
    "            if prediction[0] == row[-1]:  # Compara a label prevista com a label atual\n",
    "                correct_predictions += 1\n",
    "        \n",
    "        # Calculando a acurácia para este caso de teste\n",
    "        accuracy = correct_predictions / len(test_set)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Calcular a porcentagem de previsões corretas\n",
    "    correct_percentage = accuracies.count(1.0) / len(accuracies) * 100\n",
    "    return correct_percentage\n",
    "\n",
    "# Exemplo de chamada à função\n",
    "# Supondo que data_matrix e attributes estão definidos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb974c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = leave_one_out_cross_validation(data_matrix, attributes)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670e49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
